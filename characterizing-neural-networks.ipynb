{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\renewcommand{\\sD}{\\mathcal{D}}\n",
    "\\renewcommand{\\RR}{\\mathbb{R}}\n",
    "\\renewcommand{\\ZZ}{\\mathbb{Z}}$$\n",
    "# “On Characterizing the Capacity of Neural Networks Using Algebraic Topology”\n",
    "\n",
    "<cite data-cite=\"6503695/6JPMHH53\"></cite>\n",
    "\n",
    "- Colton Grainger\n",
    "- PhD Student, Mathematics, CU Boulder\n",
    "- repository: `github.com/coltongrainger/fy19soml`\n",
    "\n",
    "<!---\n",
    "- introduce myself\n",
    "- get oriented\n",
    "    - how well am I prepared to understand and comment on Guss's paper?\n",
    "    - what coursework would benefit me sooner than later?\n",
    "    - how are my peers and the faculty involved?\n",
    "- lay out a direction for future research\n",
    "    - why is computational topology relevant to me?\n",
    "        - it's at the intersection of pure math, applied math, cs\n",
    "        - I'm generally inspired by Nikki's work\n",
    "- make an *argument*\n",
    "- reference software\n",
    "    - ripser\n",
    "    - TDAstats\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# problem: model selection\n",
    "### key idea: consider homological complexity\n",
    "### results: topological phase transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This talk: <span style=\"color:blue\">**fully connected feed-forward networks.**</span>\n",
    "![](2019-daniely.png)\n",
    "Free parameters: <span style=\"color:blue\">**depth**</span> $\\ell$ (for *layers*) and <span style=\"color:red\">**width**</span> $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"2010-hyndman.png\" style=\"width: 1400px\"/>\n",
    "\n",
    "(Rob Hyndman, CC-BY-3.0, https://stats.stackexchange.com/q/181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  plausible approaches <cite data-cite=\"6503695/CIL5D5BD\"></cite>\n",
    "\n",
    "- trial and error\n",
    "- heuristic search\n",
    "    - *scalable?*\n",
    "- exhaustive search\n",
    "    - *computationally feasible?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### example: heuristic search <cite data-cite=\"6503695/SVFFTWZW\"></cite>\n",
    "\n",
    "(Informally) Is there a function $g \\colon \\RR^d \\to \\RR$ that\n",
    "\n",
    "- **is expressible** by a network with $\\ell$-layers and $w$ neurons per layer,\n",
    "- **yet cannot be well-approximated** by any network with $\\tilde{\\ell} < \\ell$ layers, unless the width $\\tilde{w}$ is allowed to be very large?\n",
    "\n",
    "Such a function is said to give a *depth separation*.\n",
    "\n",
    "<!---\n",
    "are such functions dense in the space of data sets? likely...\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> There are $3$-layer networks of width $w \\in O(d^k)$ polynomial in the dimension $d$ that cannot be arbitrarily well approximated by $2$-layer networks, unless the $2$-layer networks have width $\\tilde{w} \\in O(k^d)$ exponential in $d$. <cite data-cite=\"6503695/SVFFTWZW\"></cite>\n",
    "\n",
    "![](2016-eldan.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### example: exhaustive search <cite data-cite=\"6503695/6JPMHH53\"></cite>\n",
    "\n",
    "For each $\\ell$ and $h$, take\n",
    "<!---\n",
    "from which set of numbers? sheesh, this is mess.\n",
    "- depth, *number of hidden layers*, $\\ell$\n",
    "- width, *hidden units per layer*, $w$   $\\Leftrightarrow$ *number of hidden units*, $h_i$\n",
    "--->\n",
    "- fully connected architectures of $(\\ell, h)$ layers and hidden units,\n",
    "    - with unit weights initialized to samples from a normal distribution $\\mathcal{N}(0, 1/\\beta_0)$,\n",
    "    - and rectified linear activation functions.\n",
    "![](2016-guss.png) image: <cite data-cite=\"6503695/GPGK7HG7\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train architecture $(\\ell, h_0)$\n",
    "\n",
    "- for each of $930$ synthetic data distributions \n",
    "<!---\n",
    "denoted $(\\ZZ^1, \\{0\\})$ continguously up to $(\\ZZ^{30}, \\ZZ^{30})$,\n",
    "omg this looks like it's going to take *forever*\n",
    "--->\n",
    "    - take $100$ initialized instances of the architecture $(\\ell, h_0)$\n",
    "    - sample the synthetic distribution \n",
    "    - optimize against cross-entropy loss, using the Adam optimizer, a fixed learning rate, and increasing batch size. \n",
    "\n",
    "![](2018-guss-datasets.png)\n",
    "image: <cite data-cite=\"6503695/6JPMHH53\"></cite>\n",
    "\n",
    "<!---\n",
    "wtf is the difference here?\n",
    "https://machinelearning.subwiki.org/wiki/Hyperparameter_optimization\n",
    "https://machinelearning.subwiki.org/wiki/Model_class_selection\n",
    "https://machinelearning.subwiki.org/wiki/Cost_function\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### how to justify our choice of synthetic distributions?\n",
    "\n",
    "- how do we measure the error? \n",
    "- why is this an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### ~~main problem: model selection~~\n",
    "# key idea: consider homological complexity\n",
    "### results: topological phase transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](2017-hicks-transit-map.png)\n",
    "image: <cite data-cite=\"6503695/QZQ8L3PY\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](2017-hicks-transit-map-zoom.png)\n",
    "image: <cite data-cite=\"6503695/QZQ8L3PY\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## what is *persistent homology?*\n",
    "\n",
    "![](2015-topaz.png)\n",
    "image: <cite data-cite=\"6503695/5N568MB7\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### ~~main problem: model selection~~\n",
    "### ~~key idea: consider homological complexity~~\n",
    "# results: topological phase transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "> Choosing an architecture whose homological complexity matches that of the dataset, generalizes well. \n",
    ">\n",
    "> <cite data-cite=\"6503695/2IB9R6ZD\"></cite>\n",
    "\n",
    "### Software\n",
    "\n",
    "- [`ripser`](https://github.com/Ripser/ripser) <cite data-cite=\"6503695/QNHSP73F\"></cite>\n",
    "- [`TDAstats`](https://doi.org/10.21105/joss.00860) <cite data-cite=\"6503695/GMEJZPXK\"></cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "cite2c": {
   "citations": {
    "6503695/2IB9R6ZD": {
     "URL": "http://arxiv.org/abs/1805.09949",
     "abstract": "We propose the labeled Cˇ ech complex, the plain labeled Vietoris-Rips complex, and the locally scaled labeled Vietoris-Rips complex to perform persistent homology inference of decision boundaries in classiﬁcation tasks. We provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples. Our main objective is quantiﬁcation of deep neural network complexity to enable matching of datasets to pre-trained models; we report results for experiments using MNIST, FashionMNIST, and CIFAR10.",
     "accessed": {
      "day": 22,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Ramamurthy",
       "given": "Karthikeyan Natesan"
      },
      {
       "family": "Varshney",
       "given": "Kush R."
      },
      {
       "family": "Mody",
       "given": "Krishnan"
      }
     ],
     "container-title": "arXiv:1805.09949 [cs, stat]",
     "id": "6503695/2IB9R6ZD",
     "issued": {
      "day": 24,
      "month": 5,
      "year": 2018
     },
     "language": "en",
     "note": "arXiv: 1805.09949",
     "title": "Topological Data Analysis of Decision Boundaries with Application to Model Selection",
     "type": "article-journal"
    },
    "6503695/5N23IT7X": {
     "URL": "http://neuralnetworksanddeeplearning.com",
     "accessed": {
      "day": 23,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Nielsen",
       "given": "Michael A."
      }
     ],
     "id": "6503695/5N23IT7X",
     "issued": {
      "year": 2015
     },
     "language": "en",
     "title": "Neural Networks and Deep Learning",
     "type": "article-journal"
    },
    "6503695/5N568MB7": {
     "DOI": "10.1371/journal.pone.0126383",
     "URL": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126383",
     "abstract": "We apply tools from topological data analysis to two mathematical models inspired by biological aggregations such as bird flocks, fish schools, and insect swarms. Our data consists of numerical simulation output from the models of Vicsek and D'Orsogna. These models are dynamical systems describing the movement of agents who interact via alignment, attraction, and/or repulsion. Each simulation time frame is a point cloud in position-velocity space. We analyze the topological structure of these point clouds, interpreting the persistent homology by calculating the first few Betti numbers. These Betti numbers count connected components, topological circles, and trapped volumes present in the data. To interpret our results, we introduce a visualization that displays Betti numbers over simulation time and topological persistence scale. We compare our topological results to order parameters typically used to quantify the global behavior of aggregations, such as polarization and angular momentum. The topological calculations reveal events and structure not captured by the order parameters.",
     "accessed": {
      "day": 23,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Topaz",
       "given": "Chad M."
      },
      {
       "family": "Ziegelmeier",
       "given": "Lori"
      },
      {
       "family": "Halverson",
       "given": "Tom"
      }
     ],
     "container-title": "PLOS ONE",
     "container-title-short": "PLOS ONE",
     "id": "6503695/5N568MB7",
     "issue": "5",
     "issued": {
      "day": 13,
      "month": 5,
      "year": 2015
     },
     "journalAbbreviation": "PLOS ONE",
     "language": "en",
     "page": "e0126383",
     "page-first": "e0126383",
     "title": "Topological Data Analysis of Biological Aggregation Models",
     "type": "article-journal",
     "volume": "10"
    },
    "6503695/6JPMHH53": {
     "URL": "http://arxiv.org/abs/1802.04443",
     "abstract": "The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for fully-connected neural networks.",
     "accessed": {
      "day": 21,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Guss",
       "given": "William H."
      },
      {
       "family": "Salakhutdinov",
       "given": "Ruslan"
      }
     ],
     "container-title": "arXiv:1802.04443 [cs, math, stat]",
     "id": "6503695/6JPMHH53",
     "issued": {
      "day": 12,
      "month": 2,
      "year": 2018
     },
     "note": "arXiv: 1802.04443",
     "title": "On Characterizing the Capacity of Neural Networks using Algebraic Topology",
     "type": "article-journal"
    },
    "6503695/CIL5D5BD": {
     "DOI": "10.1080/01431160802549278",
     "URL": "https://www.tandfonline.com/doi/full/10.1080/01431160802549278",
     "accessed": {
      "day": 23,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Stathakis",
       "given": "D."
      }
     ],
     "container-title": "International Journal of Remote Sensing",
     "id": "6503695/CIL5D5BD",
     "issue": "8",
     "issued": {
      "day": 20,
      "month": 4,
      "year": 2009
     },
     "language": "en",
     "page": "2133-2147",
     "page-first": "2133",
     "title": "How many hidden layers and nodes?",
     "type": "article-journal",
     "volume": "30"
    },
    "6503695/GMEJZPXK": {
     "DOI": "10.21105/joss.00860",
     "URL": "http://joss.theoj.org/papers/10.21105/joss.00860",
     "abstract": "High-dimensional datasets are becoming more common in a variety of scientific fields. Well-known examples include next-generation sequencing in biology, patient health status in medicine, and computer vision in deep learning. Dimension reduction, using methods like principal component analysis (PCA), is a common preprocessing step for such datasets. However, while dimension reduction can save computing and human resources, it comes with the cost of significant information loss. Topological data analysis (TDA) aims to analyze the “shape” of high-dimensional datasets, without dimension reduction, by extracting features that are robust to small perturbations in data. Persistent features of a dataset can be used to describe it, and to compare it to other datasets. Visualization of persistent features can be done using topological barcodes or persistence diagrams (Figure 1). Application of TDA methods has granted greater insight into high-dimensional data (Lakshmikanth et al., 2017); one prominent example of this is its use to characterize a clinically relevant subgroup of breast cancer patients (Nicolau, Levine, & Carlsson, 2011). This is a particularly salient study as Nicolau et al. (2011) used a topological method, termed Progression Analysis of Disease, to identify a patient subgroup with 100% survival using that remains invisible to other clustering methods. The TDAstats R package is a comprehensive pipeline for conducting TDA. Once data is loaded into R, TDAstats can calculate, visualize, and conduct nonparametric statistical inference on persistent homology. The Ripser C++ library (Bauer, 2015), benchmarked at approximately 40 times faster than comparable software, is wrapped using Rcpp (Eddelbuettel & Francois, 2011) for efficient computation of persistent homology. TDAstats generates topological barcodes and persistence diagrams using the ubiquitous ggplot2 library (Wickham, 2016), allowing use of ggplot2 functions to manipulate plots. This reduces the number of manual steps required to prepare publication-quality figures, thus enabling reproducible research (Sandve, Nekrutenko, Taylor, & Hovig, 2013). TDAstats also implements nonparametric hypothesis testing of persistent homology using a permutation test, first described by Robinson & Turner (2017). The permutation test uses statistical resampling to approximate the distribution of the test statistic assuming the null hypothesis is true. To our knowledge, TDAstats is the first library to implement this feature in the context of topological data analysis.",
     "accessed": {
      "day": 22,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "R. Wadhwa",
       "given": "Raoul"
      },
      {
       "family": "F.K. Williamson",
       "given": "Drew"
      },
      {
       "family": "Dhawan",
       "given": "Andrew"
      },
      {
       "family": "G. Scott",
       "given": "Jacob"
      }
     ],
     "container-title": "Journal of Open Source Software",
     "id": "6503695/GMEJZPXK",
     "issue": "28",
     "issued": {
      "day": 8,
      "month": 8,
      "year": 2018
     },
     "language": "en",
     "page": "860",
     "page-first": "860",
     "shortTitle": "TDAstats",
     "title": "TDAstats: R pipeline for computing persistent homology in topological data analysis",
     "title-short": "TDAstats",
     "type": "article-journal",
     "volume": "3"
    },
    "6503695/GPGK7HG7": {
     "URL": "http://arxiv.org/abs/1612.04799",
     "abstract": "In this paper we propose a generalization of deep neural networks called deep function machines (DFMs). DFMs act on vector spaces of arbitrary (possibly inﬁnite) dimension and we show that a family of DFMs are invariant to the dimension of input data; that is, the parameterization of the model does not directly hinge on the quality of the input (eg. high resolution images). Using this generalization we provide a new theory of universal approximation of bounded non-linear operators between function spaces. We then suggest that DFMs provide an expressive framework for designing new neural network layer types with topological considerations in mind. Finally, we introduce a novel architecture, RippLeNet, for resolution invariant computer vision, which empirically achieves state of the art invariance.",
     "accessed": {
      "day": 21,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Guss",
       "given": "William H."
      }
     ],
     "container-title": "arXiv:1612.04799 [cs, stat]",
     "id": "6503695/GPGK7HG7",
     "issued": {
      "day": 14,
      "month": 12,
      "year": 2016
     },
     "language": "en",
     "note": "arXiv: 1612.04799",
     "shortTitle": "Deep Function Machines",
     "title": "Deep Function Machines: Generalized Neural Networks for Topological Layer Expression",
     "title-short": "Deep Function Machines",
     "type": "article-journal"
    },
    "6503695/QNHSP73F": {
     "URL": "https://github.com/Ripser/ripser",
     "accessed": {
      "day": 25,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Bauer",
       "given": "Ulrich"
      }
     ],
     "id": "6503695/QNHSP73F",
     "issued": {
      "day": 24,
      "month": 11,
      "year": 2018
     },
     "note": "original-date: 2015-10-27T21:43:59Z",
     "publisher": "Ripser",
     "shortTitle": "Ripser",
     "title": "Ripser: efficient computation of Vietoris–Rips persistence barcodes: Ripser/ripser",
     "title-short": "Ripser",
     "type": "book"
    },
    "6503695/QZQ8L3PY": {
     "URL": "https://math.berkeley.edu/~jhicks/classes/spring17math191/Math191Notes.pdf",
     "accessed": {
      "day": 22,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Hicks",
       "given": "Jeff"
      }
     ],
     "id": "6503695/QZQ8L3PY",
     "issued": {
      "year": 2017
     },
     "title": "Combinatorial Topology: A project oriented guide",
     "type": "manuscript"
    },
    "6503695/SVFFTWZW": {
     "URL": "http://proceedings.mlr.press/v49/eldan16.html",
     "abstract": "We show that there is a simple (approximately radial) function on \\mathbbR^d, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to mor...",
     "accessed": {
      "day": 21,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Eldan",
       "given": "Ronen"
      },
      {
       "family": "Shamir",
       "given": "Ohad"
      }
     ],
     "container-title": "Conference on Learning Theory",
     "event": "Conference on Learning Theory",
     "id": "6503695/SVFFTWZW",
     "issued": {
      "day": 6,
      "month": 6,
      "year": 2016
     },
     "language": "en",
     "page": "907-940",
     "page-first": "907",
     "title": "The Power of Depth for Feedforward Neural Networks",
     "type": "paper-conference"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
