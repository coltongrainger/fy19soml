{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\renewcommand{\\sD}{\\mathcal{D}}\n",
    "\\renewcommand{\\RR}{\\mathbb{R}}\n",
    "\\renewcommand{\\ZZ}{\\mathbb{Z}}$$\n",
    "\n",
    "# “On Characterizing the Capacity of Neural Networks Using Algebraic Topology”\n",
    "\n",
    "<cite data-cite=\"6503695/6JPMHH53\"></cite>\n",
    "\n",
    "- Colton Grainger\n",
    "- PhD Student, Mathematics, CU Boulder\n",
    "- repository: `github.com/coltongrainger/fy19soml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"2010-hyndman.png\" style=\"width: 1400px;\"/>\n",
    "\n",
    "(Rob Hyndman, CC-BY-3.0, https://stats.stackexchange.com/q/181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  plausible approaches <cite data-cite=\"6503695/CIL5D5BD\"></cite>\n",
    "\n",
    "- trial and error\n",
    "    - naive\n",
    "- heuristic search\n",
    "    - *not scalable*\n",
    "- exhaustive search\n",
    "    - could be exponential in the dimension of the problem \n",
    " \n",
    "    > There are $3$-layer networks of width polynomial in the dimension $d$, which cannot be arbitrarily well approximated by $2$-layer networks, unless their width is exponential in $d$. <cite data-cite=\"6503695/SVFFTWZW\"></cite>\n",
    "\n",
    "\n",
    "\n",
    "<!---\n",
    "even *this talk* is heuristic! \n",
    "- introduce myself\n",
    "- get oriented\n",
    "    - how well am I prepared to understand and comment on Guss's paper?\n",
    "    - what coursework would benefit me sooner than later?\n",
    "    - how are my peers and the faculty involved?\n",
    "- lay out a direction for future research\n",
    "    - why is computational topology relevant to me?\n",
    "        - it's at the intersection of pure math, applied math, cs\n",
    "        - I'm generally inspired by Nikki's work\n",
    "- make an *argument*\n",
    "- reference software\n",
    "    - ripser\n",
    "    - TDAstats\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each dimension $d$, what functions $g \\colon \\RR^d \\to \\RR$ \n",
    "- are expressible by a network with $\\ell$-layers and $w$ neurons per layer,\n",
    "- yet cannot be well-approximated by any other network with less than $\\ell$ layers, \n",
    "- *even if the number of neurons is allowed to be much larger than $w$*?\n",
    "\n",
    "![](2016-eldan.png)\n",
    "<cite data-cite=\"6503695/SVFFTWZW\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## problem: model selection\n",
    "\n",
    "Guss '18 considers **only two** hyperparameters\n",
    "\n",
    "- width or *number of hidden units*, denoted $h_i$ for some index $i$\n",
    "- depth or *number of layers*, denoted $\\ell$\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/OH3gI.png\" style=\"width: 1400px;\"/>\n",
    "\n",
    "<cite data-cite=\"6503695/5N23IT7X\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For each $\\ell$ and $h$, take\n",
    "\n",
    "- **fully connected architectures of $(\\ell, h_0)$ layers and hidden units,**\n",
    "    - **with unit weights initialized to samples from a normal distribution $\\mathcal{N}(0, 1/\\beta_0)$,**\n",
    "    - and rectified linear activation functions.\n",
    "![](2016-guss.png) image: <cite data-cite=\"6503695/GPGK7HG7\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### To train architecture $(\\ell, h_0)$\n",
    "\n",
    "- **for each synthetic dataset, denoted $(\\ZZ^1, \\{0\\})$ continguously up to $(\\ZZ^{30}, \\ZZ^{30})$**,\n",
    "    - **take $100$ initialized instances of the architecture $(\\ell, h_0)$**\n",
    "    - optimize against cross-entropy loss, using the Adam optimizer, a fixed learning rate, and increasing batch size. \n",
    "\n",
    "![](2018-guss-datasets.png)\n",
    "image: <cite data-cite=\"6503695/6JPMHH53\"></cite>\n",
    "\n",
    "<!---\n",
    "wtf is the difference here?\n",
    "https://machinelearning.subwiki.org/wiki/Hyperparameter_optimization\n",
    "https://machinelearning.subwiki.org/wiki/Model_class_selection\n",
    "https://machinelearning.subwiki.org/wiki/Cost_function\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## outline of talk so far\n",
    "- ~~problem: neural architectures need to express complex data~~\n",
    "- define homological complexity of data\n",
    "- review empirical results <cite data-cite=\"6503695/6JPMHH53\"></cite>\n",
    "\n",
    "<!---\n",
    "TODO: wording in the list above?\n",
    "--->\n",
    "\n",
    "### we want to evaluate the following conclusion <cite data-cite=\"6503695/2IB9R6ZD\"></cite> :\n",
    "## \"choose an architecture whose homological complexity matches that of the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](2017-hicks-transit-map.png)\n",
    "<cite data-cite=\"6503695/QZQ8L3PY\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](2017-hicks-transit-map-zoom.png)\n",
    "<cite data-cite=\"6503695/QZQ8L3PY\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions\n",
    "\n",
    "- simplicial complexes\n",
    "- homological algebra\n",
    "- *persistent homology*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](2015-topaz.png)\n",
    "<cite data-cite=\"6503695/5N568MB7\"></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## math\n",
    "\n",
    "$$E_H^p(f, \\sD) = \\min\\left\\{\\frac{\\beta_p(f)}{\\beta_p(\\sD)}, 1\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Failure modes\n",
    "\n",
    "Want to avoid requiring\n",
    "\n",
    "- the training data, or\n",
    "- the width of neural network\n",
    "\n",
    "to be exponential in the dimension of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- persistent homology describes data complexity \n",
    "- we want architectures that reliably express 'holes' $H(\\mathcal{D})$\n",
    "- maybe a shallow architecture does not fit the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "cite2c": {
   "citations": {
    "6503695/2IB9R6ZD": {
     "URL": "http://arxiv.org/abs/1805.09949",
     "abstract": "We propose the labeled Cˇ ech complex, the plain labeled Vietoris-Rips complex, and the locally scaled labeled Vietoris-Rips complex to perform persistent homology inference of decision boundaries in classiﬁcation tasks. We provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples. Our main objective is quantiﬁcation of deep neural network complexity to enable matching of datasets to pre-trained models; we report results for experiments using MNIST, FashionMNIST, and CIFAR10.",
     "accessed": {
      "day": 22,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Ramamurthy",
       "given": "Karthikeyan Natesan"
      },
      {
       "family": "Varshney",
       "given": "Kush R."
      },
      {
       "family": "Mody",
       "given": "Krishnan"
      }
     ],
     "container-title": "arXiv:1805.09949 [cs, stat]",
     "id": "6503695/2IB9R6ZD",
     "issued": {
      "day": 24,
      "month": 5,
      "year": 2018
     },
     "language": "en",
     "note": "arXiv: 1805.09949",
     "title": "Topological Data Analysis of Decision Boundaries with Application to Model Selection",
     "type": "article-journal"
    },
    "6503695/6JPMHH53": {
     "URL": "http://arxiv.org/abs/1802.04443",
     "abstract": "The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for fully-connected neural networks.",
     "accessed": {
      "day": 21,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Guss",
       "given": "William H."
      },
      {
       "family": "Salakhutdinov",
       "given": "Ruslan"
      }
     ],
     "container-title": "arXiv:1802.04443 [cs, math, stat]",
     "id": "6503695/6JPMHH53",
     "issued": {
      "day": 12,
      "month": 2,
      "year": 2018
     },
     "note": "arXiv: 1802.04443",
     "title": "On Characterizing the Capacity of Neural Networks using Algebraic Topology",
     "type": "article-journal"
    },
    "6503695/CIL5D5BD": {
     "DOI": "10.1080/01431160802549278",
     "URL": "https://www.tandfonline.com/doi/full/10.1080/01431160802549278",
     "accessed": {
      "day": 23,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Stathakis",
       "given": "D."
      }
     ],
     "container-title": "International Journal of Remote Sensing",
     "id": "6503695/CIL5D5BD",
     "issue": "8",
     "issued": {
      "day": 20,
      "month": 4,
      "year": 2009
     },
     "language": "en",
     "page": "2133-2147",
     "page-first": "2133",
     "title": "How many hidden layers and nodes?",
     "type": "article-journal",
     "volume": "30"
    },
    "6503695/GPGK7HG7": {
     "URL": "http://arxiv.org/abs/1612.04799",
     "abstract": "In this paper we propose a generalization of deep neural networks called deep function machines (DFMs). DFMs act on vector spaces of arbitrary (possibly inﬁnite) dimension and we show that a family of DFMs are invariant to the dimension of input data; that is, the parameterization of the model does not directly hinge on the quality of the input (eg. high resolution images). Using this generalization we provide a new theory of universal approximation of bounded non-linear operators between function spaces. We then suggest that DFMs provide an expressive framework for designing new neural network layer types with topological considerations in mind. Finally, we introduce a novel architecture, RippLeNet, for resolution invariant computer vision, which empirically achieves state of the art invariance.",
     "accessed": {
      "day": 21,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Guss",
       "given": "William H."
      }
     ],
     "container-title": "arXiv:1612.04799 [cs, stat]",
     "id": "6503695/GPGK7HG7",
     "issued": {
      "day": 14,
      "month": 12,
      "year": 2016
     },
     "language": "en",
     "note": "arXiv: 1612.04799",
     "shortTitle": "Deep Function Machines",
     "title": "Deep Function Machines: Generalized Neural Networks for Topological Layer Expression",
     "title-short": "Deep Function Machines",
     "type": "article-journal"
    },
    "6503695/SVFFTWZW": {
     "URL": "http://proceedings.mlr.press/v49/eldan16.html",
     "abstract": "We show that there is a simple (approximately radial) function on \\mathbbR^d, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to mor...",
     "accessed": {
      "day": 21,
      "month": 11,
      "year": 2018
     },
     "author": [
      {
       "family": "Eldan",
       "given": "Ronen"
      },
      {
       "family": "Shamir",
       "given": "Ohad"
      }
     ],
     "container-title": "Conference on Learning Theory",
     "event": "Conference on Learning Theory",
     "id": "6503695/SVFFTWZW",
     "issued": {
      "day": 6,
      "month": 6,
      "year": 2016
     },
     "language": "en",
     "page": "907-940",
     "page-first": "907",
     "title": "The Power of Depth for Feedforward Neural Networks",
     "type": "paper-conference"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
